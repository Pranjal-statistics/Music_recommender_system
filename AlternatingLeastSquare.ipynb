{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811f376d",
   "metadata": {},
   "source": [
    "## Alternating Least Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84444c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/ext3/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/05/16 23:36:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/16 23:36:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in /home/vr2229/.local/lib/python3.8/site-packages (12.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /ext3/pyspark/lib/python3.8/site-packages (from pyarrow) (1.21.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/vr2229/.local/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /ext3/pyspark/lib/python3.8/site-packages (from scikit-learn) (1.21.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/vr2229/.local/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vr2229/.local/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /ext3/pyspark/lib/python3.8/site-packages (from scikit-learn) (1.7.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: implicit in /home/vr2229/.local/lib/python3.8/site-packages (0.6.2)\n",
      "Requirement already satisfied: numpy in /ext3/pyspark/lib/python3.8/site-packages (from implicit) (1.21.2)\n",
      "Requirement already satisfied: scipy>=0.16 in /ext3/pyspark/lib/python3.8/site-packages (from implicit) (1.7.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vr2229/.local/lib/python3.8/site-packages (from implicit) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "'''import libraries'''\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.mllib.linalg import Matrices\n",
    "spark = SparkSession.builder.appName('listenBrainz - exploratory data analysis').getOrCreate()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "!pip install pyarrow\n",
    "!pip install scikit-learn\n",
    "!pip install implicit\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "import scipy.sparse as sparse\n",
    "import implicit\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator,RankingEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import RankingMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383854f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repartition done\n",
      "als done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/16 23:40:33 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/05/16 23:40:33 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "23/05/16 23:40:54 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "23/05/16 23:40:54 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting done\n",
      "evaluation done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1178:================================================>  (190 + 10) / 200] 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse calculated\n",
      "errors appended\n",
      "For rank 20 and regParam 0.1 the RMSE is 10.03120082205401\n",
      "The best model was trained with rank 20 and regParam 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''alternating least square'''\n",
    "train_df = spark.read.parquet(\"/home/vr2229/final-project-group-29/final_project/final-project-group-29/train_df_train.parquet\")\n",
    "val_df = spark.read.parquet(\"/home/vr2229/final-project-group-29/final_project/final-project-group-29/test.parquet\")\n",
    "\n",
    "# train_df = spark.read.parquet('/home/vr2229/final-project-group-29/final_project/final-project-group-29/train_df_small.parquet')\n",
    "# val_df = spark.read.parquet('/home/vr2229/final-project-group-29/final_project/final-project-group-29/val_df_small.parquet')\n",
    "from pyspark.sql.functions import asc\n",
    "training_df = train_df.sort(asc(\"user_id\"))\n",
    "validation_df = val_df.sort(asc(\"user_id\"))\n",
    "\n",
    "ranks = [20]\n",
    "regParam = [0.1]\n",
    "errors = []\n",
    "min_error = float('inf')\n",
    "training_df = training_df.repartition(20000)\n",
    "print('repartition done')\n",
    "\n",
    "for param in regParam:\n",
    "    for rank in ranks:\n",
    "        \n",
    "        tempALS = ALS(maxIter=20, rank=rank, regParam=param,\n",
    "              userCol='user_id', itemCol='codes', ratingCol='listens_mean', implicitPrefs = True, numUserBlocks = 10000,\n",
    "              coldStartStrategy='drop', seed=1234)\n",
    "        print('als done')\n",
    "        training_df = training_df.coalesce(20000)\n",
    "\n",
    "        model = tempALS.fit(training_df)\n",
    "        print('fitting done')\n",
    "        \n",
    "        predictions = model.transform(validation_df)\n",
    "        evaluator = RegressionEvaluator(metricName='rmse', labelCol='listens', \n",
    "                                predictionCol='prediction')\n",
    "        print('evaluation done')\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        print('rmse calculated')\n",
    "        errors.append(rmse)\n",
    "        print('errors appended')\n",
    "        print ('For rank {0} and regParam {1} the RMSE is {2}'.format (rank, param, rmse))\n",
    "        \n",
    "        if rmse < min_error:\n",
    "            min_error = rmse\n",
    "            best_rank = rank\n",
    "            best_regParam = param\n",
    "            best_model = model\n",
    "        \n",
    "print ('The best model was trained with rank {0} and regParam {1}'.format(best_rank, best_regParam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9adc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
